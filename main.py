import pandas as pd
import nltk
import numpy as np
import re
import os
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import load_model

# ----------------------------------------------------------------------
# 1. AYARLAR VE DOSYA LÄ°STESÄ° (âœ… TÃœM VERÄ° KAYNAKLARINIZ BURAYA EKLENMÄ°ÅžTÄ°R)
# ----------------------------------------------------------------------
# Proje klasÃ¶rÃ¼ndeki tÃ¼m eÄŸitim dosyalarÄ±nÄ±n adlarÄ±.
DOSYA_LISTESI = [
    'e-ticaret_urun_yorumlari.csv',  # 1. Orijinal e-ticaret verisi
    'kendi_kayitlarim.csv',  # 2. Kendi etiketlediÄŸim YouTube verisi
    'train.csv',  # 3. GeniÅŸ sosyal medya verisi

]
VEKTOR_BOYUTU = 100
EGITIM_EPOCH = 12

# ----------------------------------------------------------------------
# 2. NLTK KAYNAKLARINI Ä°NDÄ°RME
# ----------------------------------------------------------------------
try:
    print("NLTK kaynaklarÄ± kontrol ediliyor (stopwords, punkt)...")
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
except Exception as e:
    print(f"âŒ NLTK hatasÄ±: {e}")
    exit()

# ----------------------------------------------------------------------
# 3. CSV OKUMA VE ETÄ°KET STANDARTLAÅžTIRMA MOTORU
# ----------------------------------------------------------------------
print(f"\nToplam {len(DOSYA_LISTESI)} adet veri dosyasÄ± okunacak...")

df = pd.DataFrame()
toplam_okunan_dosya = 0

for dosya_adi in DOSYA_LISTESI:
    if not os.path.exists(dosya_adi):
        print(f"âš ï¸ Dosya bulunamadÄ±, atlanÄ±yor: {dosya_adi}")
        continue

    temp_df = None
    # FarklÄ± ayÄ±rÄ±cÄ± ve kodlama denemeleri
    okuma_parametreleri = [
        {'sep': ';', 'encoding': 'iso-8859-9'},
        {'sep': ',', 'encoding': 'utf-8'},
        {'sep': ',', 'encoding': 'iso-8859-9'},
        {'sep': ';', 'encoding': 'utf-8'}
    ]

    for params in okuma_parametreleri:
        try:
            temp_df = pd.read_csv(dosya_adi, **params)
            # UTF-8 BOM karakterini temizle
            temp_df.columns = temp_df.columns.str.replace('Ã¯Â»Â¿', '').str.strip()
            break
        except:
            continue

    if temp_df is not None:
        print(f"ðŸ“„ '{dosya_adi}' okundu. SÃ¼tunlar: {list(temp_df.columns)}")

        # --- SÃœTUN EÅžLEÅžTÄ°RME ---
        # Metin sÃ¼tunu iÃ§in olasÄ± isimler (Yorum, text, Yorum vb.)
        olasi_metin = ['Metin', 'text', 'yorum', 'Yorum', 'comment', 'GÃ¶rÃ¼ÅŸ', 'content', 'Text']
        # Etiket sÃ¼tunu iÃ§in olasÄ± isimler (Durum, label, Duygu vb.)
        olasi_etiket = ['Durum', 'label', 'sentiment', 'target', 'class', 'duygu', 'Label', 'Sentiment', 'Duygu']

        # OlasÄ± sÃ¼tun adlarÄ±nÄ± bul ve standart isimlerle eÅŸle
        bulunan_metin = next((col for col in temp_df.columns if col in olasi_metin), None)
        bulunan_etiket = next((col for col in temp_df.columns if col in olasi_etiket), None)

        if bulunan_metin and bulunan_etiket:
            temp_df = temp_df.rename(columns={bulunan_metin: 'Metin', bulunan_etiket: 'Durum'})
            df = pd.concat([df, temp_df[['Metin', 'Durum']]], ignore_index=True)
            print(f"   âœ… Eklendi: {len(temp_df)} satÄ±r.")
            toplam_okunan_dosya += 1
        else:
            print(f"   âŒ UYARI: '{dosya_adi}' iÃ§inde uygun Metin/Etiket sÃ¼tunu bulunamadÄ±.")
    else:
        print(f"   âŒ HATA: '{dosya_adi}' okunamadÄ± (Format hatasÄ±).")

if toplam_okunan_dosya == 0:
    print("\nâŒ HÄ°Ã‡BÄ°R VERÄ° OKUNAMADI. Program durduruluyor.")
    exit()

# Veri temizliÄŸi (BoÅŸ satÄ±rlarÄ± sil)
df = df.dropna(subset=['Metin', 'Durum'])

# --- KRÄ°TÄ°K KISIM: ETÄ°KET STANDARTLAÅžTIRMA (TÃ¼m formatlarÄ± 1/0/2'ye Ã§evirir) ---
print("\nEtiketler (Durum sÃ¼tunu) standartlaÅŸtÄ±rÄ±lÄ±yor (Text, 0, 1, 2'den -> 1/0/2'ye)...")

# Kesin EÅŸleÅŸme SÃ¶zlÃ¼ÄŸÃ¼
etiket_esleme_sozlugu = {
    # Negatif (0)
    '0': 0, 'olumsuz': 0, 'negatif': 0, 'negative': 0, 'negative': 0, 'neg': 0,
    # Pozitif (1)
    '1': 1, 'olumlu': 1, 'pozitif': 1, 'positive': 1, 'positive': 1, 'pos': 1,
    # NÃ¶tr/TarafsÄ±z (2)
    '2': 2, 'nÃ¶tr': 2, 'tarafsÄ±z': 2, 'neutral': 2, 'notr': 2, 'neu': 2
}


def etiket_duzelt_guclu(deger):
    # DeÄŸeri metne Ã§evirip kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼r ve boÅŸluklarÄ± temizle
    str_val = str(deger).strip().lower()

    if str_val in etiket_esleme_sozlugu:
        return etiket_esleme_sozlugu[str_val]

    # EÄŸer etiket hiÃ§bir ÅŸeye benzemiyorsa, varsayÄ±lan olarak NÃ¶tr (2) yap.
    return 2


df['Durum'] = df['Durum'].apply(etiket_duzelt_guclu)

# Etiketleri tam sayÄ± (integer) formatÄ±na Ã§evir
df['Durum'] = df['Durum'].astype(int)

YORUM_SUTUNU_ADI = 'Metin'
ETIKET_SUTUNU_ADI = 'Durum'
print(f"\nâœ… BÄ°RLEÅžTÄ°RME VE STANDARTLAÅžTIRMA TAMAMLANDI. Toplam EÄŸitim Verisi: {len(df)} yorum.")
# ----------------------------------------------------------------------
# ... (KODUN DEVAMI AÅžAÄžIDADIR)
# ----------------------------------------------------------------------

# ----------------------------------------------------------------------
# 4. Ã–N Ä°ÅžLEME VE WORD2VEC EÄžÄ°TÄ°MÄ°
# ----------------------------------------------------------------------
turkish_stopwords = stopwords.words('turkish')


def metin_temizle_ve_tokenlestir(metin):
    metin = str(metin).lower()
    # TÃ¼rkÃ§e harfleri ve boÅŸluklarÄ± koru
    metin = re.sub(r'[^a-zÄ±Ã¼ÅŸÃ¶Ã§ÄŸ\s]', '', metin)
    tokenler = metin.split()
    tokenler = [kelime for kelime in tokenler if kelime not in turkish_stopwords and len(kelime) > 1]
    return tokenler


print("\nYorumlar temizleniyor ve tokenleÅŸtiriliyor...")
df['temiz_tokenler'] = df[YORUM_SUTUNU_ADI].apply(metin_temizle_ve_tokenlestir)
word2vec_corpus = df['temiz_tokenler'].tolist()

print(f"\nWord2Vec modeli {VEKTOR_BOYUTU} boyutunda eÄŸitiliyor...")
word2vec_model = Word2Vec(
    sentences=word2vec_corpus,
    vector_size=VEKTOR_BOYUTU,
    window=5,
    min_count=2,
    sg=0,
    workers=4
)


def yorum_vektoru_olustur(token_listesi, model, vector_size):
    vektorler = [model.wv[kelime] for kelime in token_listesi if kelime in model.wv]
    if len(vektorler) == 0:
        return np.zeros(vector_size)
    else:
        return np.mean(vektorler, axis=0)


print("CÃ¼mle vektÃ¶rleri oluÅŸturuluyor...")
df['yorum_vektoru'] = df['temiz_tokenler'].apply(
    lambda tokens: yorum_vektoru_olustur(tokens, word2vec_model, VEKTOR_BOYUTU)
)

X = np.stack(df['yorum_vektoru'].values)
y = df[ETIKET_SUTUNU_ADI].values.astype(int)

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-Hot Encoding
NUM_CLASSES = len(np.unique(y))
print(f"Tespit edilen sÄ±nÄ±f sayÄ±sÄ±: {NUM_CLASSES}")

y_train_encoded = to_categorical(y_train, num_classes=NUM_CLASSES)
y_test_encoded = to_categorical(y_test, num_classes=NUM_CLASSES)

# ----------------------------------------------------------------------
# 5. MLP MODEL EÄžÄ°TÄ°MÄ° (DERÄ°N YAPAY SÄ°NÄ°R AÄžI)
# ----------------------------------------------------------------------
input_dim = X_train.shape[1]


def create_model_2(input_dim, num_classes):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.4),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


print(f"\n--- Model EÄŸitimi BaÅŸlÄ±yor ({EGITIM_EPOCH} epoch) ---")
model_2 = create_model_2(input_dim, NUM_CLASSES)
model_2.fit(X_train, y_train_encoded, epochs=EGITIM_EPOCH, batch_size=32, validation_split=0.1, verbose=1)

# ----------------------------------------------------------------------
# 6. DEÄžERLENDÄ°RME VE KAYIT
# ----------------------------------------------------------------------

print(f"\n--- Test SonuÃ§larÄ± ---")
loss, accuracy = model_2.evaluate(X_test, y_test_encoded, verbose=0)
print(f"Test DoÄŸruluÄŸu: {accuracy:.4f}")

y_pred_probs = model_2.predict(X_test, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test_encoded, axis=1)

print("\nSÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_true, y_pred, zero_division=0))
print("\nHata Matrisi:")
print(confusion_matrix(y_true, y_pred))

# KAYIT Ä°ÅžLEMÄ°
MODEL_DIZINI = 'kayitli_modeller'
os.makedirs(MODEL_DIZINI, exist_ok=True)

word2vec_model.save(os.path.join(MODEL_DIZINI, "word2vec_model.model"))
model_2.save(os.path.join(MODEL_DIZINI, "mlp_model_en_iyi.keras"))

print("-" * 50)
print("âœ… YENÄ°LENMÄ°Åž MODELLER BAÅžARIYLA KAYDEDÄ°LDÄ°.")
print("ðŸ‘‰ Åžimdi 'gui_scraper.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak YouTube yorumlarÄ±nÄ± analiz edebilirsiniz.")
print("-" * 50)

# main.py dosyasÄ±nda, model_2'nin eÄŸitimi bittikten sonra ekleyin:

print("\n--- Yapay Sinir AÄŸÄ± Topolojisi (Model Ã–zeti) ---")
model_2.summary()